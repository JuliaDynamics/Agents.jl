<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Predator-Prey Model with Reinforcement Learning · Agents.jl</title><meta name="title" content="Predator-Prey Model with Reinforcement Learning · Agents.jl"/><meta property="og:title" content="Predator-Prey Model with Reinforcement Learning · Agents.jl"/><meta property="twitter:title" content="Predator-Prey Model with Reinforcement Learning · Agents.jl"/><meta name="description" content="Documentation for Agents.jl."/><meta property="og:description" content="Documentation for Agents.jl."/><meta property="twitter:description" content="Documentation for Agents.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="https://fonts.googleapis.com/css?family=Montserrat|Source+Code+Pro&amp;display=swap" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Agents.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Agents.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Introduction</a></li><li><a class="tocitem" href="../../tutorial/">Tutorial</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../sir/">SIR model for the spread of COVID-19</a></li><li><a class="tocitem" href="../flock/">Flocking model</a></li><li><a class="tocitem" href="../zombies/">Zombie Outbreak in a City</a></li><li><a class="tocitem" href="../predator_prey/">Predator-prey dynamics</a></li><li><a class="tocitem" href="../rabbit_fox_hawk/">3D Mixed-Agent Ecosystem with Pathfinding</a></li><li><a class="tocitem" href="../event_rock_paper_scissors/">Spatial rock-paper-scissors (event based)</a></li><li><a class="tocitem" href="../rl_boltzmann/">Boltzmann Wealth Model with Reinforcement Learning</a></li><li><a class="tocitem" href="../">More Examples for Agents.jl</a></li></ul></li><li><a class="tocitem" href="../../api/">API</a></li><li><a class="tocitem" href="../agents_visualizations/">Plotting and Interactivity</a></li><li><span class="tocitem">Ecosystem Integration</span><ul><li><a class="tocitem" href="../optim/">BlackBoxOptim.jl</a></li><li><a class="tocitem" href="../diffeq/">DifferentialEquations.jl</a></li><li><a class="tocitem" href="../schoolyard/">Graphs.jl</a></li><li><a class="tocitem" href="../celllistmap/">CellListMap.jl</a></li><li><a class="tocitem" href="../delaunay/">DelaunayTriangulation.jl</a></li><li><a class="tocitem" href="../measurements/">Uncertanty Propagation</a></li></ul></li><li><a class="tocitem" href="../../performance_tips/">Performance Tips</a></li><li><a class="tocitem" href="../../comparison/">ABM Framework Comparison</a></li><li><a class="tocitem" href="../../devdocs/">Developer Docs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Predator-Prey Model with Reinforcement Learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Predator-Prey Model with Reinforcement Learning</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/Agents.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDynamics/Agents.jl/blob/main/examples/rl_wolfsheep.jl" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Predator-Prey-Model-with-Reinforcement-Learning"><a class="docs-heading-anchor" href="#Predator-Prey-Model-with-Reinforcement-Learning">Predator-Prey Model with Reinforcement Learning</a><a id="Predator-Prey-Model-with-Reinforcement-Learning-1"></a><a class="docs-heading-anchor-permalink" href="#Predator-Prey-Model-with-Reinforcement-Learning" title="Permalink"></a></h1><p>This example demonstrates how to integrate reinforcement learning (RL) with the classic predator-prey model. Building on the traditional Wolf-Sheep model, this version replaces random movement with learned behavior, where agents use reinforcement learning to optimize their survival and reproduction strategies.</p><p>The model showcases how RL agents can learn complex behaviors in multi-species ecosystems, with wolves learning to hunt efficiently and sheep learning to avoid predators while foraging for grass.</p><h2 id="Model-specification"><a class="docs-heading-anchor" href="#Model-specification">Model specification</a><a id="Model-specification-1"></a><a class="docs-heading-anchor-permalink" href="#Model-specification" title="Permalink"></a></h2><p>This model extends the classic predator-prey dynamics with reinforcement learning:</p><p><strong>Environment:</strong></p><ul><li>2D periodic grid with grass that regrows over time</li><li>Wolves hunt sheep for energy</li><li>Sheep eat grass for energy</li><li>Both species can reproduce when they have sufficient energy</li></ul><p><strong>RL Integration:</strong></p><ul><li><strong>Actions</strong>: Stay, move North, South, East, or West (5 discrete actions)</li><li><strong>Observations</strong>: Local neighborhood information including other agents, grass, and own energy</li><li><strong>Rewards</strong>: Survival, energy maintenance, successful feeding, and reproduction</li><li><strong>Goal</strong>: Learn optimal movement and foraging/hunting strategies</li></ul><p><strong>Key differences from traditional model:</strong></p><ul><li>Movement decisions are learned rather than random</li><li>Agents can develop sophisticated strategies over time</li><li>Emergent behaviors arise from individual learning rather than hard-coded rules</li></ul><h2 id="Loading-packages-and-defining-agent-types"><a class="docs-heading-anchor" href="#Loading-packages-and-defining-agent-types">Loading packages and defining agent types</a><a id="Loading-packages-and-defining-agent-types-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-packages-and-defining-agent-types" title="Permalink"></a></h2><pre><code class="language-julia hljs">using Agents, Random, Statistics, POMDPs, Crux, Flux, Distributions

@agent struct RLSheep(GridAgent{2})
    energy::Float64
    reproduction_prob::Float64
    Δenergy::Float64
end

@agent struct RLWolf(GridAgent{2})
    energy::Float64
    reproduction_prob::Float64
    Δenergy::Float64
end</code></pre><h2 id="Agent-stepping-functions"><a class="docs-heading-anchor" href="#Agent-stepping-functions">Agent stepping functions</a><a id="Agent-stepping-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Agent-stepping-functions" title="Permalink"></a></h2><p>The stepping functions define how agents behave in response to RL actions. Unlike the traditional model with random movement, here movement is determined by the RL policy based on the learned strategy.</p><h3 id="Sheep-stepping-function"><a class="docs-heading-anchor" href="#Sheep-stepping-function">Sheep stepping function</a><a id="Sheep-stepping-function-1"></a><a class="docs-heading-anchor-permalink" href="#Sheep-stepping-function" title="Permalink"></a></h3><p>Sheep must balance energy conservation, grass foraging, and predator avoidance.</p><pre><code class="language-julia hljs"># Wolf-sheep RL step functions
function sheepwolf_step_rl!(sheep::RLSheep, model, action::Int)
    # Action definitions: 1=stay, 2=north, 3=south, 4=east, 5=west
    current_x, current_y = sheep.pos
    width, height = getfield(model, :space).extent

    dx, dy = 0, 0
    if action == 2      # North
        dy = 1
    elseif action == 3  # South
        dy = -1
    elseif action == 4  # East
        dx = 1
    elseif action == 5  # West
        dx = -1
    end

    # Apply periodic boundary wrapping and move
    if action != 1  # If not staying
        new_x = mod1(current_x + dx, width)
        new_y = mod1(current_y + dy, height)
        target_pos = (new_x, new_y)
        move_agent!(sheep, target_pos, model)
    end

    # Energy decreases with each step (movement cost)
    sheep.energy -= 1
    if sheep.energy &lt; 0
        remove_agent!(sheep, model)
        return
    end

    # Try to eat grass if available
    if model.fully_grown[sheep.pos...]
        sheep.energy += sheep.Δenergy
        model.fully_grown[sheep.pos...] = false
        model.countdown[sheep.pos...] = model.regrowth_time
    end

    # Reproduce if energy is sufficient
    if rand(abmrng(model)) ≤ sheep.reproduction_prob
        sheep.energy /= 2
        replicate!(sheep, model)
    end
end</code></pre><h3 id="Wolf-stepping-function"><a class="docs-heading-anchor" href="#Wolf-stepping-function">Wolf stepping function</a><a id="Wolf-stepping-function-1"></a><a class="docs-heading-anchor-permalink" href="#Wolf-stepping-function" title="Permalink"></a></h3><p>Wolves must learn efficient hunting strategies while managing their energy reserves.</p><pre><code class="language-julia hljs"># WOLF Step
function sheepwolf_step_rl!(wolf::RLWolf, model, action::Int)
    # Action definitions: 1=stay, 2=north, 3=south, 4=east, 5=west
    current_x, current_y = wolf.pos
    width, height = getfield(model, :space).extent

    dx, dy = 0, 0
    if action == 2      # North
        dy = 1
    elseif action == 3  # South
        dy = -1
    elseif action == 4  # East
        dx = 1
    elseif action == 5  # West
        dx = -1
    end

    # Apply periodic boundary wrapping and move
    if action != 1  # If not staying
        new_x = mod1(current_x + dx, width)
        new_y = mod1(current_y + dy, height)
        move_agent!(wolf, (new_x, new_y), model)
    end

    # Energy decreases with each step
    wolf.energy -= 1
    if wolf.energy &lt; 0
        remove_agent!(wolf, model)
        return
    end

    # Hunt sheep if available at current position
    sheep_ids = [id for id in ids_in_position(wolf.pos, model) if haskey(model.agents, id) &amp;&amp; model[id] isa RLSheep]
    if !isempty(sheep_ids)
        dinner = model[sheep_ids[1]]
        remove_agent!(dinner, model)
        wolf.energy += wolf.Δenergy
    end

    # Reproduce if energy is sufficient
    if rand(abmrng(model)) ≤ wolf.reproduction_prob
        wolf.energy /= 2
        replicate!(wolf, model)
    end
end</code></pre><h3 id="Grass-dynamics-and-unified-stepping"><a class="docs-heading-anchor" href="#Grass-dynamics-and-unified-stepping">Grass dynamics and unified stepping</a><a id="Grass-dynamics-and-unified-stepping-1"></a><a class="docs-heading-anchor-permalink" href="#Grass-dynamics-and-unified-stepping" title="Permalink"></a></h3><p>Grass regrows over time, providing a renewable resource for sheep.</p><pre><code class="language-julia hljs">function grass_step!(model)
    @inbounds for p in positions(model)
        if !(model.fully_grown[p...])  # If grass is not fully grown
            if model.countdown[p...] ≤ 0
                model.fully_grown[p...] = true  # Regrow grass
            else
                model.countdown[p...] -= 1  # Countdown to regrowth
            end
        end
    end
end

# Unified stepping function for both agent types
function wolfsheep_rl_step!(agent::Union{RLSheep,RLWolf}, model, action::Int)
    if agent isa RLSheep
        sheepwolf_step_rl!(agent, model, action)
    elseif agent isa RLWolf
        sheepwolf_step_rl!(agent, model, action)
    end

    # Stochastic grass regrowth
    if rand(abmrng(model)) &lt; 0.6
        grass_step!(model)
    end
end

function agent_wolfsheep_rl_step!(agent::Union{RLSheep,RLWolf}, model, action::Int)
    if agent isa RLSheep
        sheepwolf_step_rl!(agent, model, action)
    elseif agent isa RLWolf
        sheepwolf_step_rl!(agent, model, action)
    end
end</code></pre><h2 id="RL-specific-functions"><a class="docs-heading-anchor" href="#RL-specific-functions">RL-specific functions</a><a id="RL-specific-functions-1"></a><a class="docs-heading-anchor-permalink" href="#RL-specific-functions" title="Permalink"></a></h2><p>The following functions define how the RL environment interacts with the ABM:</p><ul><li><strong>Observation function</strong>: Provides agents with local environmental information</li><li><strong>Reward function</strong>: Shapes learning by rewarding desired behaviors</li><li><strong>Terminal function</strong>: Determines when episodes end</li></ul><h3 id="Observation-function"><a class="docs-heading-anchor" href="#Observation-function">Observation function</a><a id="Observation-function-1"></a><a class="docs-heading-anchor-permalink" href="#Observation-function" title="Permalink"></a></h3><p>Agents observe their local neighborhood, including other agents, grass availability, and their own status. This information helps them make informed decisions.</p><pre><code class="language-julia hljs"># Wolf-sheep observation function
function get_local_observation(model::ABM, agent_id::Int, observation_radius::Int)
    target_agent = model[agent_id]
    agent_pos = target_agent.pos
    width, height = getfield(model, :space).extent
    agent_type = target_agent isa RLSheep ? :sheep : :wolf

    grid_size = 2 * observation_radius + 1
    # 3 channels: sheep, wolves, grass
    neighborhood_grid = zeros(Float32, grid_size, grid_size, 3, 1)

    # Get valid neighboring agents
    neighbor_ids = nearby_ids(target_agent, model, observation_radius)
    valid_neighbors = []
    for id in neighbor_ids
        if haskey(model.agents, id) &amp;&amp; id != agent_id
            push!(valid_neighbors, model[id])
        end
    end

    # Map neighbors to observation grid
    for neighbor in valid_neighbors
        dx = neighbor.pos[1] - agent_pos[1]
        dy = neighbor.pos[2] - agent_pos[2]

        # Handle periodic boundaries
        if abs(dx) &gt; width / 2
            dx -= sign(dx) * width
        end
        if abs(dy) &gt; height / 2
            dy -= sign(dy) * height
        end

        grid_x = dx + observation_radius + 1
        grid_y = dy + observation_radius + 1

        if 1 &lt;= grid_x &lt;= grid_size &amp;&amp; 1 &lt;= grid_y &lt;= grid_size
            if neighbor isa RLSheep
                neighborhood_grid[grid_x, grid_y, 1, 1] = 1.0  # Sheep channel
            elseif neighbor isa RLWolf
                neighborhood_grid[grid_x, grid_y, 2, 1] = 1.0  # Wolf channel
            end
        end
    end

    # Add grass information to observation
    for dx in -observation_radius:observation_radius
        for dy in -observation_radius:observation_radius
            pos_x = mod1(agent_pos[1] + dx, width)
            pos_y = mod1(agent_pos[2] + dy, height)

            grid_x = dx + observation_radius + 1
            grid_y = dy + observation_radius + 1

            if model.fully_grown[pos_x, pos_y]
                neighborhood_grid[grid_x, grid_y, 3, 1] = 1.0  # Grass channel
            end
        end
    end

    # Normalize agent&#39;s own information
    normalized_energy = Float32(target_agent.energy / 40.0)
    normalized_pos = (Float32(agent_pos[1] / width), Float32(agent_pos[2] / height))

    return (
        agent_id=agent_id,
        agent_type=agent_type,
        own_energy=normalized_energy,
        normalized_pos=normalized_pos,
        neighborhood_grid=neighborhood_grid
    )
end

# Convert observation to vector format for neural networks
function wolfsheep_get_observation(model, agent_id, observation_radius)
    observation_data = get_local_observation(model, agent_id, observation_radius)

    # Flatten spatial information
    flattened_grid = vec(observation_data.neighborhood_grid)

    # Combine all features into a single observation vector
    return vcat(
        Float32(observation_data.own_energy),
        Float32(observation_data.normalized_pos[1]),
        Float32(observation_data.normalized_pos[2]),
        Float32(observation_data.agent_type == :sheep ? 1.0 : 0.0),  # Agent type indicator
        flattened_grid
    )
end</code></pre><h3 id="Reward-function"><a class="docs-heading-anchor" href="#Reward-function">Reward function</a><a id="Reward-function-1"></a><a class="docs-heading-anchor-permalink" href="#Reward-function" title="Permalink"></a></h3><p>The reward function shapes agent learning by providing feedback on their actions. Different strategies are used for sheep (survival and foraging) vs wolves (hunting).</p><pre><code class="language-julia hljs"># Define reward function
function wolfsheep_calculate_reward(env, agent, action, initial_model, final_model)
    # Death penalty - strongest negative reward
    if agent.id ∉ [a.id for a in allagents(final_model)]
        return -50.0
    end

    if agent isa RLSheep
        # Sheep rewards: survival, energy maintenance, successful foraging
        reward = 1.0  # Base survival bonus

        # Energy level bonus (normalized)
        energy_ratio = agent.energy / 20.0
        reward += energy_ratio * 0.5

        # Bonus for successful foraging (energy increase)
        if haskey(initial_model.agents, agent.id)
            initial_energy = initial_model[agent.id].energy
            if agent.energy &gt; initial_energy
                reward += 0.5  # Foraging success bonus
            end
        end

        return reward

    else  # Wolf
        # Wolf rewards: survival, energy maintenance, successful hunting
        reward = 1.0  # Base survival bonus

        # Energy level bonus (wolves can have higher energy)
        energy_ratio = agent.energy / 40.0
        reward += energy_ratio * 0.3

        # Large bonus for successful hunting (significant energy increase)
        if haskey(initial_model.agents, agent.id)
            initial_energy = initial_model[agent.id].energy
            if agent.energy &gt; initial_energy + 10  # Indicates successful hunt
                reward += 0.5  # Hunting success bonus
            end
        end

        return reward
    end
end</code></pre><h3 id="Terminal-condition"><a class="docs-heading-anchor" href="#Terminal-condition">Terminal condition</a><a id="Terminal-condition-1"></a><a class="docs-heading-anchor-permalink" href="#Terminal-condition" title="Permalink"></a></h3><p>Episodes end when either species goes extinct, creating natural stopping points for learning episodes while maintaining ecological realism.</p><pre><code class="language-julia hljs"># Define terminal condition for RL model
function wolfsheep_is_terminal_rl(env)
    sheep_count = length([a for a in allagents(env) if a isa RLSheep])
    wolf_count = length([a for a in allagents(env) if a isa RLWolf])
    return sheep_count == 0 || wolf_count == 0
end</code></pre><h2 id="Model-initialization"><a class="docs-heading-anchor" href="#Model-initialization">Model initialization</a><a id="Model-initialization-1"></a><a class="docs-heading-anchor-permalink" href="#Model-initialization" title="Permalink"></a></h2><p>The following functions handle model creation and RL configuration setup, similar to the traditional wolf-sheep model but with RL capabilities.</p><pre><code class="language-julia hljs">function create_fresh_wolfsheep_model(n_sheeps, n_wolves, dims, regrowth_time, Δenergy_sheep,
    Δenergy_wolf, sheep_reproduce, wolf_reproduce, seed)

    rng = MersenneTwister(seed)
    space = GridSpace(dims, periodic=true)

    # Model properties for grass dynamics
    properties = Dict{Symbol,Any}(
        :fully_grown =&gt; falses(dims),
        :countdown =&gt; zeros(Int, dims),
        :regrowth_time =&gt; regrowth_time,
    )

    # Create the ReinforcementLearningABM
    model = ReinforcementLearningABM(Union{RLSheep,RLWolf}, space;
        agent_step=agent_wolfsheep_rl_step!, model_step=grass_step!,
        properties=properties, rng=rng,
        scheduler=Schedulers.Randomly())

    # Add sheep agents
    for _ in 1:n_sheeps
        energy = rand(abmrng(model), 1:(Δenergy_sheep*2)) - 1
        add_agent!(RLSheep, model, energy, sheep_reproduce, Δenergy_sheep)
    end

    # Add wolf agents
    for _ in 1:n_wolves
        energy = rand(abmrng(model), 1:(Δenergy_wolf*2)) - 1
        add_agent!(RLWolf, model, energy, wolf_reproduce, Δenergy_wolf)
    end

    # Initialize grass with random growth states
    for p in positions(model)
        fully_grown = rand(abmrng(model), Bool)
        countdown = fully_grown ? regrowth_time : rand(abmrng(model), 1:regrowth_time) - 1
        model.countdown[p...] = countdown
        model.fully_grown[p...] = fully_grown
    end

    return model
end

# Initialize model function for RL ABM
function initialize_rl_model(; n_sheeps=30, n_wolves=5, dims=(10, 10), regrowth_time=10,
    Δenergy_sheep=5, Δenergy_wolf=20, sheep_reproduce=0.2, wolf_reproduce=0.05,
    observation_radius=4, seed=1234)

    # RL configuration specifying learning environment parameters
    rl_config = RLConfig(;
        model_init_fn = () -&gt; create_fresh_wolfsheep_model(n_sheeps, n_wolves, dims, regrowth_time,
            Δenergy_sheep, Δenergy_wolf, sheep_reproduce, wolf_reproduce, seed),
        observation_fn = wolfsheep_get_observation,
        reward_fn = wolfsheep_calculate_reward,
        terminal_fn = wolfsheep_is_terminal_rl,
        agent_step_fn = wolfsheep_rl_step!,
        action_spaces = Dict(
            RLSheep =&gt; Crux.DiscreteSpace(5),  # 5 movement actions
            RLWolf =&gt; Crux.DiscreteSpace(5)    # 5 movement actions
        ),
        observation_spaces = Dict(
            RLSheep =&gt; Crux.ContinuousSpace((((2 * observation_radius + 1)^2 * 3) + 4,), Float32),
            RLWolf =&gt; Crux.ContinuousSpace((((2 * observation_radius + 1)^2 * 3) + 4,), Float32)
        ),
        training_agent_types = [RLSheep, RLWolf],
        max_steps = 300,
        observation_radius = observation_radius,
        discount_rates = Dict(
            RLSheep =&gt; 0.99,  # Long-term planning for survival
            RLWolf =&gt; 0.99    # Long-term planning for hunting
        )
    )

    # Create the model and set RL configuration
    model = create_fresh_wolfsheep_model(n_sheeps, n_wolves, dims, regrowth_time, Δenergy_sheep,
        Δenergy_wolf, sheep_reproduce, wolf_reproduce, seed)

    set_rl_config!(model, rl_config)

    return model
end</code></pre><h2 id="Training-the-RL-agents"><a class="docs-heading-anchor" href="#Training-the-RL-agents">Training the RL agents</a><a id="Training-the-RL-agents-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-RL-agents" title="Permalink"></a></h2><p>Now we create the model and train both sheep and wolves simultaneously. This creates a co-evolutionary dynamic where both species adapt to each other.</p><pre><code class="language-julia hljs"># Create the model
rl_model = initialize_rl_model(n_sheeps=50, n_wolves=10, dims=(20, 20), regrowth_time=30,
    Δenergy_sheep=4, Δenergy_wolf=20, sheep_reproduce=0.04, wolf_reproduce=0.05, seed=1234)

println(&quot;Created ReinforcementLearningABM with $(nagents(rl_model)) agents&quot;)
println(&quot;Sheep: $(length([a for a in allagents(rl_model) if a isa RLSheep]))&quot;)
println(&quot;Wolves: $(length([a for a in allagents(rl_model) if a isa RLWolf]))&quot;)</code></pre><p>Train both species simultaneously</p><pre><code class="language-julia hljs">println(&quot;\nTraining wolves and sheep with reinforcement learning...&quot;)
try
    train_model!(rl_model,:simultaneous;  # Both species learn at the same time
        n_iterations=5,
        batch_size=400 * nagents(rl_model),
        solver_params=Dict(
            :ΔN =&gt; 100 * nagents(rl_model),
            :log =&gt; (period=100 * nagents(rl_model),),
            :max_steps =&gt; 200 * nagents(rl_model)
        ))
    println(&quot;Training completed successfully&quot;)
catch e
    println(&quot;Training failed with error: $e&quot;)
    rethrow(e)
end</code></pre><h2 id="Running-the-trained-model"><a class="docs-heading-anchor" href="#Running-the-trained-model">Running the trained model</a><a id="Running-the-trained-model-1"></a><a class="docs-heading-anchor-permalink" href="#Running-the-trained-model" title="Permalink"></a></h2><p>After training, we create a fresh model instance and apply the learned policies to observe how the trained agents behave in the predator-prey ecosystem.</p><pre><code class="language-julia hljs"># Create a fresh model instance for simulation
println(&quot;\nCreating fresh Wolf-Sheep model for simulation...&quot;)
fresh_ws_model = initialize_rl_model(n_sheeps=50, n_wolves=10, dims=(20, 20), regrowth_time=30,
    Δenergy_sheep=4, Δenergy_wolf=20, sheep_reproduce=0.04, wolf_reproduce=0.05, seed=1234)

# Copy the trained policies to the fresh model
copy_trained_policies!(fresh_ws_model, rl_model)
println(&quot;Applied trained policies to fresh model&quot;)</code></pre><h2 id="Visualization"><a class="docs-heading-anchor" href="#Visualization">Visualization</a><a id="Visualization-1"></a><a class="docs-heading-anchor-permalink" href="#Visualization" title="Permalink"></a></h2><p>Let&#39;s visualize the ecosystem and observe the learned behaviors.</p><pre><code class="language-julia hljs">using CairoMakie, ColorSchemes
CairoMakie.activate!()

# Define colors and markers for different agent types
function agent_color(agent)
    if agent isa RLSheep
        return :lightblue  # Sheep are light blue
    elseif agent isa RLWolf
        return :red        # Wolves are red
    else
        return :black      # Fallback color
    end
end

function agent_marker(agent)
    if agent isa RLSheep
        return :circle     # Sheep are circles
    elseif agent isa RLWolf
        return :rect       # Wolves are squares
    else
        return :circle     # Fallback marker
    end
end

# Plot the initial state
fig, ax = abmplot(fresh_ws_model;
    agent_color=agent_color,
    agent_marker=agent_marker
)
display(fig)</code></pre><p>Run simulation with trained agents</p><pre><code class="language-julia hljs">println(&quot;\nRunning simulation with trained RL agents...&quot;)
initial_sheep = length([a for a in allagents(fresh_ws_model) if a isa RLSheep])
initial_wolves = length([a for a in allagents(fresh_ws_model) if a isa RLWolf])
println(&quot;Initial populations - Sheep: $initial_sheep, Wolves: $initial_wolves&quot;)

# Step the model forward to observe trained behavior
try
    Agents.step!(fresh_ws_model, 200)
    println(&quot;Simulation completed successfully&quot;)
catch e
    println(&quot;Simulation failed with error: $e&quot;)
    rethrow(e)
end

# Check final population numbers
final_sheep = length([a for a in allagents(fresh_ws_model) if a isa RLSheep])
final_wolves = length([a for a in allagents(fresh_ws_model) if a isa RLWolf])

println(&quot;Population changes after 200 steps:&quot;)
println(&quot;Sheep: $initial_sheep → $final_sheep&quot;)
println(&quot;Wolves: $initial_wolves → $final_wolves&quot;)

# Analyze the results
if final_sheep &gt; 0 &amp;&amp; final_wolves &gt; 0
    println(&quot;Success! Both species coexist - predator-prey balance maintained&quot;)
elseif final_sheep == 0
    println(&quot;Wolves were too successful - sheep went extinct&quot;)
elseif final_wolves == 0
    println(&quot;Sheep outlasted wolves - predators died out&quot;)
end</code></pre><h2 id="Creating-an-animation"><a class="docs-heading-anchor" href="#Creating-an-animation">Creating an animation</a><a id="Creating-an-animation-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-an-animation" title="Permalink"></a></h2><p>Create a video showing the trained ecosystem dynamics over time.</p><pre><code class="language-julia hljs">fresh_ws_model = initialize_rl_model(n_sheeps=50, n_wolves=10, dims=(20, 20), regrowth_time=30,
    Δenergy_sheep=4, Δenergy_wolf=20, sheep_reproduce=0.04, wolf_reproduce=0.05, seed=1234)

# Copy the trained policies to the fresh model
copy_trained_policies!(fresh_ws_model, rl_model)

plotkwargs = (
    agent_color=agent_color,
    agent_marker=agent_marker,
)
abmvideo(&quot;wolfsheep_rl.mp4&quot;, fresh_ws_model; frames=100,
    framerate=2,
    title=&quot;Wolf-Sheep Model with RL - Blue=Sheep, Red=Wolves&quot;,
    plotkwargs...)</code></pre><h2 id="Key-takeaways"><a class="docs-heading-anchor" href="#Key-takeaways">Key takeaways</a><a id="Key-takeaways-1"></a><a class="docs-heading-anchor-permalink" href="#Key-takeaways" title="Permalink"></a></h2><p>This example demonstrates several important concepts:</p><ol><li><p><strong>Multi-agent RL</strong>: Both predator and prey species learn simultaneously, creating co-evolutionary dynamics where each species adapts to the other.</p></li><li><p><strong>Complex reward structures</strong>: Different reward functions for different agent types (survival for sheep, hunting for wolves) lead to emergent ecological behaviors.</p></li><li><p><strong>Spatial awareness</strong>: Agents learn to use local environmental information (locations of prey/predators, grass availability) to make strategic decisions.</p></li><li><p><strong>Emergent strategies</strong>: Trained agents may develop sophisticated behaviors like flocking (sheep), pursuit strategies (wolves), or territorial behaviors.</p></li><li><p><strong>Ecosystem dynamics</strong>: The learned behaviors can lead to more realistic predator-prey cycles compared to purely random movement models.</p></li></ol></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 30 January 2026 15:15">Friday 30 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
